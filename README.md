# ğŸ† Quantum Credit Risk Prediction

**Banco Santander Quantum Hackathon 2025 - Madrid**

---

## ğŸ“‹ Table of Contents

1. [The Challenge](#-the-challenge)
2. [Data Pipeline](#-data-pipeline)
3. [Approach 1: VQC](#-approach-1-vqc-variational-quantum-classifier)
4. [Approach 2: QSVC](#-approach-2-qsvc-quantum-support-vector-classifier)
5. [Why QSVC Excels in Small-Data Regime](#-why-qsvc-excels-in-small-data-regime)
6. [Quick Start](#-quick-start)
7. [Project Files](#-project-files)

---

## ğŸ¯ The Challenge

| Aspect | Description |
|--------|-------------|
| **Task** | Binary classification - predict loan default (Yes/No) |
| **Dataset** | Credit risk data with financial and personal features |
| **Goal** | Leverage quantum computing for credit risk assessment |

---

## ğŸ“ˆ Data Pipeline

Both quantum approaches share the same preprocessing pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data    â”‚â”€â”€â”€â”€â–¶â”‚   Encode     â”‚â”€â”€â”€â”€â–¶â”‚     PCA      â”‚â”€â”€â”€â”€â–¶â”‚  Normalize   â”‚
â”‚  12 features â”‚     â”‚ Categoricals â”‚     â”‚   11 â†’ 5     â”‚     â”‚   [0, Ï€]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 1: Handle Missing Values & Outliers

```python
df['person_emp_length'].fillna(df['person_emp_length'].mode()[0])
df['loan_int_rate'].fillna(df['loan_int_rate'].median())
df = df[df['person_age'] <= 100]
```

### Step 2: Encode Categorical Variables

| Variable | Encoding |
|----------|----------|
| `person_home_ownership` | LabelEncoder |
| `loan_grade` | Ordinal (A=0...G=6) |
| `loan_intent` | LabelEncoder |
| `cb_person_default_on_file` | Binary (N=0, Y=1) |

### Step 3: PCA Reduction

```python
# 11 features â†’ 5 principal components
pca = PCA(n_components=5)
X_pca = pca.fit_transform(StandardScaler().fit_transform(X))
```

**Why PCA?**
- Reduces dimensionality to match qubit count
- Captures maximum variance in fewer components
- Eliminates feature correlations

### Step 4: Normalize to [0, Ï€]

```python
X_quantum = MinMaxScaler(feature_range=(0, np.pi)).fit_transform(X_pca)
```

**Why [0, Ï€]?** Quantum rotation gates RY(Î¸) have period 2Ï€. Scaling to [0, Ï€] ensures full rotation range without redundancy.

---

## âš›ï¸ Approach 1: VQC (Variational Quantum Classifier)

### Circuit Architecture

```
       ENCODING              VARIATIONAL LAYER           MEASUREMENT
          â”‚                         â”‚                         â”‚
q0: â”€â”€[RY(xâ‚€)]â”€â”€â”€â”€[RY(Î¸â‚€)]â”€â”€[RZ(Î¸â‚)]â”€â”€â—â”€â”€â”€â”€[RY(Î¸â‚‚)]â”€â”€â”€â”€â”€â”€â”€â”€â”€< Z >
                                      â”‚
q1: â”€â”€[RY(xâ‚)]â”€â”€â”€â”€[RY(Î¸â‚ƒ)]â”€â”€[RZ(Î¸â‚„)]â”€â”€Zâ”€â”€â—â”€â”€[RY(Î¸â‚…)]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                         â”‚
q2: â”€â”€[RY(xâ‚‚)]â”€â”€â”€â”€[RY(Î¸â‚†)]â”€â”€[RZ(Î¸â‚‡)]â”€â”€â”€â”€â”€Zâ”€â”€â—â”€â”€[RY(Î¸â‚ˆ)]â”€â”€â”€â”€â”€â”€â”€
                                            â”‚
q3: â”€â”€[RY(xâ‚ƒ)]â”€â”€â”€â”€[RY(Î¸â‚‰)]â”€â”€[RZ(Î¸â‚â‚€)]â”€â”€â”€â”€â”€â”€â”€Zâ”€â”€â—â”€â”€[RY(Î¸â‚â‚)]â”€â”€â”€
                                               â”‚
q4: â”€â”€[RY(xâ‚„)]â”€â”€â”€â”€[RY(Î¸â‚â‚‚)]â”€â”€[RZ(Î¸â‚â‚ƒ)]â”€â”€â”€â”€â”€â”€â”€â”€â”€Zâ”€â”€[RY(Î¸â‚â‚„)]â”€â”€â”€
```

### How It Works

1. **Encoding**: Each PCA component â†’ RY rotation on corresponding qubit
2. **Variational Layer**: Trainable RY/RZ rotations + CZ entanglement
3. **Measurement**: âŸ¨ZâŸ© expectation value on qubit 0
4. **Training**: Optimize Î¸ parameters via gradient descent (Parameter Shift Rule)
5. **Decision**: âŸ¨ZâŸ© > threshold â†’ No Default, else â†’ Default

---

## ğŸ”¬ Approach 2: QSVC (Quantum Support Vector Classifier)

### The Core Idea: Quantum Kernel

QSVC uses the quantum circuit not to classify directly, but to **measure similarity** between data points. This similarity function (kernel) is then used by a classical SVM.

### Mathematical Foundation

The quantum kernel is based on **state fidelity**:

$$K(x, y) = |\langle\phi(x)|\phi(y)\rangle|^2$$

Where:
- $|Ï†(x)âŸ© = U(x)|0âŸ©$ is the quantum state encoding data point x
- $U(x)$ is the feature map circuit
- $K(x,y)$ measures how similar two quantum states are

**Key Property**: If $x â‰ˆ y$, then $U(x) â‰ˆ U(y)$, so $Uâ€ (y)U(x) â‰ˆ I$, thus $K(x,y) â‰ˆ 1$

### Computing the Kernel: Circuit Design

To compute $K(x,y)$, we apply $U(x)$ followed by $Uâ€ (y)$ and measure:

$$K(x, y) = |\langle 0|U^\dagger(y) \cdot U(x)|0\rangle|^2 = P(|00000\rangle)$$

```
         U(x) - Feature Map              Uâ€ (y) - Adjoint              MEASURE
              â”‚                               â”‚                          â”‚
              â”‚                               â”‚                          â”‚
q0: â”€â”€[RY(xâ‚€)]â”€â”€[RZ(xâ‚€)]â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[RZ(-yâ‚€)]â”€â”€[RY(-yâ‚€)]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ P(|0âŸ©)
                          â”‚
q1: â”€â”€[RY(xâ‚)]â”€â”€[RZ(xâ‚)]â”€â”€Xâ”€â”€â—â”€â”€â”€â”€â”€â”€â”€[RZ(-yâ‚)]â”€â”€[RY(-yâ‚)]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ P(|0âŸ©)
                             â”‚
q2: â”€â”€[RY(xâ‚‚)]â”€â”€[RZ(xâ‚‚)]â”€â”€â”€â”€â”€Xâ”€â”€â—â”€â”€â”€â”€[RZ(-yâ‚‚)]â”€â”€[RY(-yâ‚‚)]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ P(|0âŸ©)
                                â”‚
q3: â”€â”€[RY(xâ‚ƒ)]â”€â”€[RZ(xâ‚ƒ)]â”€â”€â”€â”€â”€â”€â”€â”€Xâ”€â”€â—â”€[RZ(-yâ‚ƒ)]â”€â”€[RY(-yâ‚ƒ)]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ P(|0âŸ©)
                                   â”‚
q4: â”€â”€[RY(xâ‚„)]â”€â”€[RZ(xâ‚„)]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Xâ”€[RZ(-yâ‚„)]â”€â”€[RY(-yâ‚„)]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ P(|0âŸ©)
```

### Step-by-Step: Encoding to Kernel Value

#### Step 1: Feature Map U(x)

For each data point x = [xâ‚€, xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„]:

```python
def feature_map(x):
    for layer in range(N_LAYERS):
        # Rotation layer - encode features
        for i in range(N_QUBITS):
            qml.RY(x[i], wires=i)           # Amplitude encoding
            qml.RZ(x[i] * scale, wires=i)   # Phase encoding
        
        # Entanglement layer - create correlations
        for i in range(N_QUBITS - 1):
            qml.CNOT(wires=[i, i + 1])
        qml.CNOT(wires=[N_QUBITS - 1, 0])   # Circular connection
```

This transforms $|00000âŸ©$ â†’ $|Ï†(x)âŸ©$

#### Step 2: Adjoint Feature Map Uâ€ (y)

Apply the **inverse** operations in **reverse order**:

```python
def adjoint_feature_map(y):
    for layer in reversed(range(N_LAYERS)):
        # Reverse entanglement
        qml.CNOT(wires=[N_QUBITS - 1, 0])
        for i in reversed(range(N_QUBITS - 1)):
            qml.CNOT(wires=[i, i + 1])
        
        # Reverse rotations (negative angles)
        for i in range(N_QUBITS):
            qml.RZ(-y[i] * scale, wires=i)
            qml.RY(-y[i], wires=i)
```

#### Step 3: Measure Kernel Value

```python
@qml.qnode(dev)
def kernel_circuit(x, y):
    feature_map(x)           # Apply U(x)
    adjoint_feature_map(y)   # Apply Uâ€ (y)
    return qml.probs(wires=range(N_QUBITS))

def kernel_value(x, y):
    probs = kernel_circuit(x, y)
    return probs[0]  # P(|00000âŸ©) = K(x, y)
```

### Building the Kernel Matrix

For training, we need the similarity between **all pairs** of training points:

```
              xâ‚    xâ‚‚    xâ‚ƒ   ...   xâ‚™
           â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
      xâ‚   â”‚ 1.0 â”‚ 0.8 â”‚ 0.3 â”‚ ... â”‚ 0.5 â”‚
           â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
      xâ‚‚   â”‚ 0.8 â”‚ 1.0 â”‚ 0.4 â”‚ ... â”‚ 0.6 â”‚
           â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
K_train =  xâ‚ƒ   â”‚ 0.3 â”‚ 0.4 â”‚ 1.0 â”‚ ... â”‚ 0.2 â”‚
           â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
      ...  â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚
           â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
      xâ‚™   â”‚ 0.5 â”‚ 0.6 â”‚ 0.2 â”‚ ... â”‚ 1.0 â”‚
           â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

**Properties:**
- Diagonal â‰ˆ 1.0 (each point is identical to itself)
- Symmetric: K(x,y) = K(y,x)
- Off-diagonal: similarity between different points

```python
def compute_kernel_matrix(X1, X2, symmetric=False):
    n1, n2 = len(X1), len(X2)
    K = np.zeros((n1, n2))
    
    for i in range(n1):
        for j in range(i if symmetric else 0, n2):
            K[i, j] = kernel_value(X1[i], X2[j])
            if symmetric:
                K[j, i] = K[i, j]
    return K
```

### Training the QSVC

Once we have the kernel matrix, training is **classical**:

```python
from sklearn.svm import SVC

# 1. Compute quantum kernel matrices
K_train = compute_kernel_matrix(X_train, X_train, symmetric=True)
K_test = compute_kernel_matrix(X_test, X_train, symmetric=False)

# 2. Normalize kernel (optional but recommended)
d = np.sqrt(np.diag(K_train))
K_train_norm = K_train / np.outer(d, d)
K_test_norm = K_test / d

# 3. Train SVM with precomputed kernel
svm = SVC(kernel='precomputed', C=best_C, class_weight='balanced')
svm.fit(K_train_norm, y_train)

# 4. Predict
y_pred = svm.predict(K_test_norm)
```

### Hyperparameter Tuning

The SVM parameter **C** controls regularization. We tune it **without recomputing the kernel**:

```python
def tune_C(K_train, y_train):
    best_auc, best_C = 0, 1.0
    for C in [0.1, 1.0, 10.0, 100.0]:
        svm = SVC(kernel='precomputed', C=C, probability=True)
        svm.fit(K_train, y_train)
        # Cross-validate or use validation set
        if auc > best_auc:
            best_auc, best_C = auc, C
    return best_C
```

### Inference: New Clients

When a new client applies for a loan:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         NEW CLIENT INFERENCE                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  NEW CLIENT DATA
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚ age: 28, income: 45000, home: "RENT", ...
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: PREPROCESS (same pipeline as training)                                   â”‚
â”‚                                                                                  â”‚
â”‚   Raw â†’ Encode â†’ StandardScaler â†’ PCA â†’ MinMaxScaler[0,Ï€]                       â”‚
â”‚                                                                                  â”‚
â”‚   x_new = [0.82, 1.45, 2.31, 0.67, 1.89]  (5 values in [0, Ï€])                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: COMPUTE KERNEL VECTOR                                                    â”‚
â”‚                                                                                  â”‚
â”‚   For each training point xáµ¢:                                                    â”‚
â”‚     k_new[i] = kernel_circuit(x_new, xáµ¢)  â†’  P(|00000âŸ©)                         â”‚
â”‚                                                                                  â”‚
â”‚   k_new = [0.45, 0.72, 0.31, 0.88, ...]  (n_train values)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: SVM PREDICTION                                                           â”‚
â”‚                                                                                  â”‚
â”‚   prediction = svm.predict(k_new)                                                â”‚
â”‚   probability = svm.predict_proba(k_new)                                         â”‚
â”‚                                                                                  â”‚
â”‚   Output: class=0 (No Default), P(Default)=0.23                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: DECISION                                                                 â”‚
â”‚                                                                                  â”‚
â”‚   if P(Default) < threshold:                                                     â”‚
â”‚       APPROVE LOAN                                                               â”‚
â”‚   else:                                                                          â”‚
â”‚       REJECT LOAN (or require further review)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Why QSVC Excels in Small-Data Regime

### 1. Kernel Methods Are Data-Efficient

Unlike neural networks that need massive datasets, **kernel methods** (including SVM) are designed to work well with limited data. They find decision boundaries using only the most informative points (support vectors).

### 2. Exponentially Large Feature Space

With 5 qubits, the quantum kernel implicitly operates in a **2âµ = 32 dimensional** Hilbert space. This rich representation can capture complex patterns that would require many more features classically.

```
Original space:    5 dimensions (PCA components)
Quantum space:    32 dimensions (quantum amplitudes)
```

### 3. Entanglement Captures Feature Interactions

The CNOT gates create **quantum correlations** between qubits. This means the kernel naturally captures interactions between features (e.g., income Ã— loan amount) that classical kernels like RBF cannot efficiently represent.

### 4. No Parameters to Overfit

Unlike VQC which has trainable parameters (Î¸), **QSVC has no quantum parameters to train**. The feature map is fixed. This eliminates the risk of overfitting the quantum circuit to the training data.

### 5. Implicit Regularization

The quantum kernel provides a form of **implicit regularization**. The structure of the quantum circuit constrains which similarity functions are possible, preventing the model from fitting noise.

### 6. Theoretical Foundations

Research has shown that quantum kernels can provide **provable advantages** for certain data distributions. Credit risk data, with its complex feature interactions and class imbalance, appears to benefit from the quantum kernel's properties.

---

## ğŸš€ Quick Start

### Installation

```bash
conda env create -f environment.yml
conda activate iqm
```

### Run VQC

```bash
python quantum_credit_risk_vqc.py
```

### Run QSVC

```bash
python quantum_credit_risk_qsvc.py
```

---

## ğŸ“ Project Files

| File | Description |
|------|-------------|
| `quantum_credit_risk_vqc.py` | VQC implementation |
| `quantum_credit_risk_qsvc.py` | QSVC implementation |
| `environment.yml` | Conda environment |

---

*Developed for Banco Santander Quantum Hackathon 2025 - Madrid*
